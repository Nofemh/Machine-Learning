{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e56391e",
   "metadata": {},
   "source": [
    "# Implementing Ensemble Learning using Scikit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9626277c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f1ce70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the diabetes dataset\n",
    "\n",
    "df = pd.read_csv(\"diabetes.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a0569b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting features (X) and target (y)\n",
    "X = df.iloc[:,:-1].values\n",
    "y = df.iloc[:,8].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216ab4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4c8f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a decision tree base model\n",
    "base_model = DecisionTreeClassifier(criterion='entropy',\n",
    "                                    max_depth=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e47509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the base_model classifier\n",
    "base_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the performance\n",
    "basetrainAcc = base_model.score(X_train, y_train)\n",
    "print(\"Training Accuracy:\", basetrainAcc)\n",
    "basetestAcc = base_model.score(X_test, y_test)\n",
    "print(\"Testing Accuracy:\", basetestAcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d260c0",
   "metadata": {},
   "source": [
    "## BaggingClassifier "
   ]
  },
  {
   "cell_type": "raw",
   "id": "1bf79c58",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html\n",
    "\n",
    "class sklearn.ensemble.BaggingClassifier(estimator=None, n_estimators=10, *, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=None, random_state=None, verbose=0)\n",
    "\n",
    "estimator: object, default=None\n",
    "The base estimator to fit on random subsets of the dataset. If None, then the base estimator is a DecisionTreeClassifier.\n",
    "\n",
    "n_estimators: int, default=10\n",
    "The number of base estimators in the ensemble.\n",
    "\n",
    "bootstrap: bool, default=True\n",
    "Whether samples are drawn with replacement. If False, sampling without replacement is performed.\n",
    "\n",
    "oob_score: bool, default=False\n",
    "Whether to use out-of-bag samples to estimate the generalization error. Only available if bootstrap=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7bacea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the bagging classifier\n",
    "bagging_clf = BaggingClassifier(base_estimator=base_model,\n",
    "                                n_estimators=32,\n",
    "                                random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb12bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the bagging classifier\n",
    "bagging_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415eca5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance\n",
    "trainbagAcc = bagging_clf.score(X_train, y_train)\n",
    "print(\"Training Accuracy:\", trainbagAcc)\n",
    "testbagAcc = bagging_clf.score(X_test, y_test)\n",
    "print(\"Testing Accuracy:\", testbagAcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce97eb5",
   "metadata": {},
   "source": [
    "## Using OOB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756ae620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the bagging classifier\n",
    "bagging_clf2 = BaggingClassifier(base_estimator=base_model,\n",
    "                                n_estimators=32,\n",
    "                                random_state=42,\n",
    "                                oob_score=True)\n",
    "# Train the bagging classifier\n",
    "bagging_clf2.fit(X, y)\n",
    "# Evaluate the performance\n",
    "trainbagAcc2 = bagging_clf2.score(X, y)\n",
    "print(\"Training Accuracy:\", trainbagAcc2)\n",
    "#testbagAcc = bagging_clf.score(X_test, y_test)\n",
    "#print(\"Testing Accuracy:\", testbagAcc)\n",
    "oobaccuracy2 = bagging_clf2.oob_score_\n",
    "print(\"OOB Accuracy:\", oobaccuracy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6838edc2",
   "metadata": {},
   "source": [
    "## Using Pasting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8688515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the pasting classifier\n",
    "pasting_clf = BaggingClassifier(base_estimator=base_model,\n",
    "                                n_estimators=32,\n",
    "                                random_state=42,\n",
    "                                bootstrap=False)\n",
    "\n",
    "# Train the pasting classifier\n",
    "pasting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the performance\n",
    "trainpastingAcc = pasting_clf.score(X_train, y_train)\n",
    "print(\"Training Accuracy:\", trainpastingAcc)\n",
    "testpastingAcc = pasting_clf.score(X_test, y_test)\n",
    "print(\"Testing Accuracy:\", testpastingAcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eb996a",
   "metadata": {},
   "source": [
    "## Bagging - Random Forests "
   ]
  },
  {
   "cell_type": "raw",
   "id": "27dffead",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "class sklearn.ensemble.RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='sqrt', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None, monotonic_cst=None)\n",
    "\n",
    "n_estimators: int, default=100\n",
    "The number of trees in the forest.\n",
    "\n",
    "criterion: {“gini”, “entropy”, “log_loss”}, default=”gini”\n",
    "The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “log_loss” and “entropy” both for the Shannon information gain, see Mathematical formulation. Note: This parameter is tree-specific.\n",
    "\n",
    "max_depth: int, default=None\n",
    "The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dee7a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf = RandomForestClassifier(n_estimators=100,\n",
    "                            criterion='gini',\n",
    "                            random_state=42)\n",
    "\n",
    "# Train the Random Forest classifier\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the performance\n",
    "trainrfAcc = rf.score(X_train, y_train)\n",
    "print(\"Training Accuracy:\", trainrfAcc)\n",
    "testrfAcc = rf.score(X_test, y_test)\n",
    "print(\"Testing Accuracy:\", testrfAcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb215a03",
   "metadata": {},
   "source": [
    "## Boosting - AdaBoost "
   ]
  },
  {
   "cell_type": "raw",
   "id": "24946ea5",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "\n",
    "estimator: object, default=None\n",
    "The base estimator from which the boosted ensemble is built. Support for sample weighting is required, as well as proper classes_ and n_classes_ attributes. If None, then the base estimator is DecisionTreeClassifier initialized with max_depth=1.\n",
    "\n",
    "n_estimators: int, default=50\n",
    "The maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early. Values must be in the range (1, inf).\n",
    "\n",
    "learning_rate: float, default=1.0\n",
    "Weight applied to each classifier at each boosting iteration. A higher learning rate increases the contribution of each classifier. There is a trade-off between the learning_rate and n_estimators parameters. Values must be in the range (0.0, inf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e9a1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a stump model\n",
    "stump = DecisionTreeClassifier(criterion='entropy',\n",
    "                                    max_depth=1)\n",
    "\n",
    "# Train the stump classifier\n",
    "stump.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the performance\n",
    "stumptrainAcc = stump.score(X_train, y_train)\n",
    "print(\"Training Accuracy:\", stumptrainAcc)\n",
    "stumptestAcc = stump.score(X_test, y_test)\n",
    "print(\"Accuracy:\", stumptestAcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01586656",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Initialize the AdaBoost classifier\n",
    "ada_clf = AdaBoostClassifier(base_estimator=stump,\n",
    "                             n_estimators=100,\n",
    "                             learning_rate=0.15,\n",
    "                             random_state=42)\n",
    "\n",
    "# Train the AdaBoost classifier\n",
    "ada_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the performance\n",
    "adatrainAcc = ada_clf.score(X_train, y_train)\n",
    "print(\"Training Accuracy:\", adatrainAcc)\n",
    "adatestAcc = ada_clf.score(X_test, y_test)\n",
    "print(\"Accuracy:\", adatestAcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60303f53",
   "metadata": {},
   "source": [
    "## Stacking "
   ]
  },
  {
   "cell_type": "raw",
   "id": "4a2ec4c2",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingClassifier.html\n",
    "\n",
    "class sklearn.ensemble.StackingClassifier(estimators, final_estimator=None, *, cv=None, stack_method='auto', n_jobs=None, passthrough=False, verbose=0)\n",
    "\n",
    "estimators: list of (str, estimator)\n",
    "Base estimators which will be stacked together. Each element of the list is defined as a tuple of string (i.e. name) and an estimator instance. An estimator can be set to ‘drop’ using set_params.\n",
    "\n",
    "final_estimator: estimator, default=None\n",
    "A classifier which will be used to combine the base estimators. The default classifier is a LogisticRegression.\n",
    "\n",
    "cv: int, cross-validation generator, iterable, or “prefit”, default=None\n",
    "Determines the cross-validation splitting strategy used in cross_val_predict to train final_estimator. Possible inputs for cv are:\n",
    "\n",
    "1. None, to use the default 5-fold cross validation,\n",
    "\n",
    "2. integer, to specify the number of folds in a (Stratified) KFold,\n",
    "\n",
    "3. An object to be used as a cross-validation generator,\n",
    "\n",
    "4. An iterable yielding train, test splits,\n",
    "\n",
    "5. \"prefit\" to assume the estimators are prefit. In this case, the estimators will not be refitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2116e919",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "raw",
   "id": "90184f93",
   "metadata": {},
   "source": [
    "Machine learning pipelines consist of multiple sequential steps that do everything from data extraction and preprocessing to model training and deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94da6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_models = [\n",
    "    ('KNN', make_pipeline(MinMaxScaler(), KNeighborsClassifier())),\n",
    "    ('SVC', make_pipeline(MinMaxScaler(), SVC())),\n",
    "    ('Adaboost', AdaBoostClassifier()),\n",
    "    ('RF', RandomForestClassifier())\n",
    "    ]\n",
    "stacked = StackingClassifier(\n",
    "    estimators = base_models,\n",
    "    final_estimator = LogisticRegression(),\n",
    "    cv = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07864aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in base_models:\n",
    "    model.fit(X_train, y_train)\n",
    "    prediction = model.predict(X_test)\n",
    "    \n",
    "    trainacc = model.score(X_train, y_train)\n",
    "    acc = model.score(X_test, y_test)\n",
    "    f1 = f1_score(y_test, prediction, average='weighted')\n",
    "    \n",
    "    print(\"-------{}-------\".format(name))\n",
    "    print(\"Training Accuracy:\",trainacc)\n",
    "    print(\"Testing Accuracy:\",acc)\n",
    "    print(\"F1-score:\",f1)\n",
    "    print(\"----------------------------------\\n\")\n",
    "\n",
    "stacked.fit(X_train, y_train)    \n",
    "stacked_prediction = stacked.predict(X_test)\n",
    "\n",
    "stacked_trainacc = stacked.score(X_train, y_train)\n",
    "stacked_acc = stacked.score(X_test, y_test)\n",
    "stacked_f1 = f1_score(y_test, stacked_prediction, average='weighted')\n",
    "print(\"-------Stacked Ensemble-------\")\n",
    "print(\"Training Accuracy: {}\".format(stacked_trainacc))\n",
    "print(\"Testing Accuracy: {}\".format(stacked_acc))\n",
    "print(\"F1-score: {}\".format(stacked_f1))\n",
    "print(\"----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fbdbee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
