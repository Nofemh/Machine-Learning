{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 - Supervised Learning\n",
    "##  Breast Cancer Diagnosis Classification \n",
    "![BreastCancer](https://archive.ics.uci.edu/ml/assets/MLimages/Large14.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Set Information:\n",
    "\n",
    "This is a subset from UCI Breast Cancer Wisconsin (Diagnostic) Data Set [Link](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic))\n",
    "\n",
    "Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.\n",
    "\n",
    "\n",
    "\n",
    "### Attribute Information:\n",
    "\n",
    "Label\n",
    " - Diagnosis (1 = M = malignant, 0 = B = benign)\n",
    " \n",
    " \n",
    "Features 1-5 : Five real-valued features are computed for each cell nucleus:\n",
    "\n",
    "- a) radius (mean of distances from center to points on the perimeter)\n",
    "- b) texture (standard deviation of gray-scale values)\n",
    "- c) perimeter\n",
    "- d) area\n",
    "- e) smoothness (local variation in radius lengths)\n",
    "\n",
    "### Main Task:\n",
    "Breast Cancer Prediction (binary classification of the diagnosis (M/B) using the classification models we have coverd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_curve, auc, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Load the data set and have a general overview on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BreastCancer=pd.read_csv('/home/nofe/lms/ds/breastcancerdata.csv')\n",
    "BreastCancer.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BreastCancer.set_index('mean_radius', inplace = True)\n",
    "BreastCancer.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Split the dataset into 75% Training and 25& Testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(BreastCancer.drop('diagnosis', axis=1), \n",
    "                                                    BreastCancer['diagnosis'], test_size=0.25, \n",
    "                                                    random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Task 3: Try all the coverd classification models (you may use others):\n",
    "- print summary classification report\n",
    "- plot Normalized confusion matrix\n",
    "- plot ROC Curve (you may use [plot_roc_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_roc_curve.html) function from sklearn instead of doing it manually)\n",
    "- print the accuracy_score & roc_auc_score scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression\n",
    "reg= LogisticRegression()\n",
    "reg.fit(X_train,y_train)\n",
    "pred=reg.predict(X_test)\n",
    "print(confusion_matrix(y_test,pred))\n",
    "print(classification_report(y_test,pred))\n",
    "print(accuracy_score(y_test,pred))\n",
    "#Normalized confusion matrix\n",
    "print ('Normalized confusion matrix')\n",
    "plot_confusion_matrix(reg, X_test, y_test, normalize='true', cmap ='orange')  \n",
    "plt.show() \n",
    "y_score = reg.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test,y_score)\n",
    "roc_auc = roc_auc_score(y_test,y_score)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # random predictions curve\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate or (1 - Specifity)')\n",
    "plt.ylabel('True Positive Rate or (Sensitivity)')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "# 4-print the accuracy_score & roc_auc_score scores\n",
    "\n",
    "lg_a = accuracy_score(y_test,pred)\n",
    "lg_r = (roc_auc)\n",
    "\n",
    "print('accuracy score= %.2f' % lg_a)\n",
    "print ('roc aue score = %.2f' % lg_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#knn\n",
    "knn = KNeighborsClassifier(n_neighbors=2)\n",
    "knn.fit(X_train,y_train)\n",
    "pred = knn.predict(X_test)\n",
    "#Printing Confusiob matrix, classification report and accuracy report \n",
    "print(confusion_matrix(y_test,pred))\n",
    "print(classification_report(y_test,pred))\n",
    "print(accuracy_score(y_test,pred))\n",
    "# Compute fpr, tpr, thresholds and roc auc\n",
    "y_score = knn.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test,y_score)\n",
    "roc_auc = roc_auc_score(y_test,y_score)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # random predictions curve\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate or (1 - Specifity)')\n",
    "plt.ylabel('True Positive Rate or (Sensitivity)')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "KNN_a = accuracy_score(y_test,pred)\n",
    "KNN_r = (roc_auc)\n",
    "\n",
    "print('accuracy score= %.2f' % KNN_a)\n",
    "print ('roc aue score = %.2f' % KNN_r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deciosn tree\n",
    "DT = DecisionTreeClassifier(random_state=42).fit(X_train, y_train)\n",
    "pred = DT.predict(X_test)\n",
    "#Normalized confusion matrix\n",
    "print ('Normalized confusion matrix')\n",
    "plot_confusion_matrix(DT, X_test, y_test, normalize='true', cmap ='Blues')  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute fpr, tpr, thresholds and roc auc\n",
    "y_score = DT.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test,y_score)\n",
    "roc_auc = roc_auc_score(y_test,y_score)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # random predictions curve\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate or (1 - Specifity)')\n",
    "plt.ylabel('True Positive Rate or (Sensitivity)')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "Decision_Tree_a = accuracy_score(y_test,pred)\n",
    "Decision_Tree_r = (roc_auc)\n",
    "print('accuracy score= %.2f' % Decision_Tree_a)\n",
    "print ('roc aue score = %.2f' % Decision_Tree_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forset\n",
    "RF = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "RF.fit(X_train, y_train)\n",
    "pred = RF.predict(X_test)\n",
    "#Printing Confusiob matrix, classification report and accuracy report \n",
    "print(confusion_matrix(y_test,pred))\n",
    "print(classification_report(y_test,pred))\n",
    "print(accuracy_score(y_test,pred))\n",
    "#Normalized confusion matrix\n",
    "print ('Normalized confusion matrix')\n",
    "plot_confusion_matrix(RF, X_test, y_test, normalize='true', cmap ='Blues')  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute fpr, tpr, thresholds and roc auc\n",
    "y_score = RF.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test,y_score)\n",
    "roc_auc = roc_auc_score(y_test,y_score)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # random predictions curve\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate or (1 - Specifity)')\n",
    "plt.ylabel('True Positive Rate or (Sensitivity)')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "Random_Forest_a = accuracy_score(y_test,pred)\n",
    "Random_Forest_r = (roc_auc)\n",
    "print('accuracy score= %.2f' % Random_Forest_a)\n",
    "print ('roc aue score = %.2f' % Random_Forest_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making the object of GNB\n",
    "\n",
    "GNB = GaussianNB()\n",
    "GNB.fit(X_train, y_train)\n",
    "pred = GNB.predict(X_test)\n",
    "#Printing Confusiob matrix, classification report and accuracy report \n",
    "print(confusion_matrix(y_test,pred))\n",
    "print(classification_report(y_test,pred))\n",
    "print(accuracy_score(y_test,pred))\n",
    "\n",
    "#Normalized confusion matrix\n",
    "print ('Normalized confusion matrix')\n",
    "plot_confusion_matrix(GNB, X_test, y_test, normalize='true', cmap ='Blues')  \n",
    "plt.show() \n",
    "# Compute fpr, tpr, thresholds and roc auc\n",
    "y_score = GNB.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test,y_score)\n",
    "roc_auc = roc_auc_score(y_test,y_score)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # random predictions curve\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate or (1 - Specifity)')\n",
    "plt.ylabel('True Positive Rate or (Sensitivity)')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "Naive_Bayes_a = accuracy_score(y_test,pred)\n",
    "Naive_Bayes_r = (roc_auc)\n",
    "print('accuracy score= %.2f' % Naive_Bayes_a)\n",
    "print ('roc aue score = %.2f' % Naive_Bayes_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SVM needs features to be scaled\n",
    "#Creating SVM model\n",
    "SVC = make_pipeline(StandardScaler(),\n",
    "                    SVC(probability=True))\n",
    "\n",
    "SVC.fit(X_train, y_train)\n",
    "pred = SVC.predict(X_test)\n",
    "#Printing Confusiob matrix, classification report and accuracy report \n",
    "print(confusion_matrix(y_test,pred))\n",
    "print(classification_report(y_test,pred))\n",
    "print(accuracy_score(y_test,pred))\n",
    "# Compute fpr, tpr, thresholds and roc auc\n",
    "y_score = SVC.predict_proba(X_test)[::,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test,y_score)\n",
    "roc_auc = roc_auc_score(y_test,y_score)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.3f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')  # random predictions curve\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate or (1 - Specifity)')\n",
    "plt.ylabel('True Positive Rate or (Sensitivity)')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "SVM_a = accuracy_score(y_test,pred)\n",
    "SVM_r = (roc_auc)\n",
    "print('accuracy score= %.2f' % SVM_a)\n",
    "print ('roc aue score = %.2f' % SVM_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Compare all models results:\n",
    "- in one cell, print all accuracy and auc scores\n",
    "- which model has the hieghest accuracy and whic has the heighest auc?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('for regreission:')\n",
    "print('accuracy score= %.2f' % lg_a)\n",
    "print ('roc aue score = %.2f' % lg_r)\n",
    "print('for knn:')\n",
    "print('accuracy score= %.2f' % KNN_a)\n",
    "print ('roc aue score = %.2f' % KNN_r)\n",
    "print('for decision tree:')\n",
    "print('accuracy score= %.2f' % Decision_Tree_a)\n",
    "print ('roc aue score = %.2f' % Decision_Tree_r)\n",
    "print('for random forest:')\n",
    "print('accuracy score= %.2f' % Random_Forest_a)\n",
    "print ('roc aue score = %.2f' % Random_Forest_r)\n",
    "print('for naive bayes :')\n",
    "print('accuracy score= %.2f' % Naive_Bayes_a)\n",
    "print ('roc aue score = %.2f' % Naive_Bayes_r)\n",
    "print('for svm :')\n",
    "print('accuracy score= %.2f' % SVM_a)\n",
    "print ('roc aue score = %.2f' % SVM_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # which model has the hieghest accuracy and which has the heighest auc?\n",
    " Random Foreast and SVM have the highest accuracy\n",
    " Naive Bayes and SVM have the highest auc score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: For cancer diagnosis problems, which metric do you think is more important, accuracy or auc? why? depending on your choice, which model would you consider the best?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy is better because when comparing the performance of machine learning algorithms, AUC is considered to be a more appropriate performance evaluation indicator than accuracy.\n",
    "so for medical data anylsis and such the more accurate the better .\n",
    "the AUC is used with probabilities in order to analyze the prediction more deeply.which diagnostics needs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
